{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYkFluTmLTZBcBo7BJZEeK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/CS909/blob/master/nn_optimization_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0N5m-XdJBUq",
        "outputId": "9b5b076f-3c1e-4900-d340-ce166914be6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "                    Strategy  Mean Epochs    CI Lower    CI Upper\n",
            "0           Base Model (SGD)   500.000000  500.000000  500.000000\n",
            "1             Adam Optimizer    65.700000   15.700000  131.400000\n",
            "2  Add Weight Initialization   500.000000  500.000000  500.000000\n",
            "3          Gradient Clipping    66.933333   16.666667  133.600000\n",
            "4           Skip Connections   134.500000   51.166667  216.666667\n",
            "5        Batch Normalization    29.433333    0.000000   75.567500\n",
            "6                 Leaky ReLU    22.866667    0.000000   62.400000\n",
            "7               One Cycle LR     4.800000    0.000000   14.400000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# XOR dataset\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32).to(device)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32).to(device)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Different strategies implemented in model variants\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self, use_batch_norm=False, use_skip_connections=False, init_method=None, activation=F.relu, deeper=False):\n",
        "        super(XORNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)\n",
        "        self.fc2 = nn.Linear(4, 8)\n",
        "        self.fc3 = nn.Linear(8, 4)\n",
        "        self.fc4 = nn.Linear(4, 1)\n",
        "\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.activation = activation\n",
        "        self.deeper = deeper\n",
        "\n",
        "        # Batch Normalization\n",
        "        if use_batch_norm:\n",
        "            self.bn1 = nn.BatchNorm1d(4)\n",
        "            self.bn2 = nn.BatchNorm1d(8)\n",
        "            self.bn3 = nn.BatchNorm1d(4)\n",
        "\n",
        "        # Skip connection\n",
        "        if use_skip_connections:\n",
        "            self.skip = nn.Linear(2, 4)\n",
        "\n",
        "        # Weight initialization\n",
        "        if init_method:\n",
        "            init_method(self.fc1.weight)\n",
        "            init_method(self.fc2.weight)\n",
        "            init_method(self.fc3.weight)\n",
        "            init_method(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_skip_connections:\n",
        "            x_skip = self.skip(x)\n",
        "        x = self.fc1(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc3(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn3(x)\n",
        "        if self.use_skip_connections:\n",
        "            x = x + x_skip\n",
        "        x = self.activation(x)\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "# Function to train and return the epoch reaching the target error\n",
        "def train_model(model, optimizer_class=optim.SGD, lr_scheduler=None, max_norm=None, epochs=500, target_error=0.00001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optimizer_class(model.parameters(), lr=0.01)\n",
        "    if lr_scheduler:\n",
        "        scheduler = lr_scheduler(optimizer)\n",
        "    for epoch in range(epochs):\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            if max_norm:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            optimizer.step()\n",
        "        if lr_scheduler:\n",
        "            scheduler.step()\n",
        "        if loss.item() <= target_error:\n",
        "            return epoch\n",
        "    return epochs\n",
        "\n",
        "# Define strategies with Adam optimizer for all except the baseline\n",
        "strategies = {\n",
        "    'Base Model (SGD)': (XORNet(deeper=True), optim.SGD),\n",
        "    'Adam Optimizer': (XORNet(deeper=True), optim.Adam),\n",
        "    'Add Weight Initialization': (XORNet(deeper=True, init_method=nn.init.xavier_uniform_), optim.Adam),\n",
        "    'Gradient Clipping': (XORNet(deeper=True), optim.Adam),\n",
        "    'Skip Connections': (XORNet(deeper=True, use_skip_connections=True), optim.Adam),\n",
        "    'Batch Normalization': (XORNet(deeper=True, use_batch_norm=True), optim.Adam),\n",
        "    'Leaky ReLU': (XORNet(deeper=True, activation=F.leaky_relu), optim.Adam),\n",
        "    'One Cycle LR': (XORNet(deeper=True), optim.Adam)\n",
        "}\n",
        "\n",
        "# Run each model multiple times and record average epochs to target error\n",
        "results = []\n",
        "num_runs = 30\n",
        "target_error = 0.00001\n",
        "\n",
        "for name, (model, optimizer_class) in strategies.items():\n",
        "    epochs_to_target = []\n",
        "    for _ in range(num_runs):\n",
        "        if name == 'Gradient Clipping':\n",
        "            epochs = train_model(model, optimizer_class, max_norm=1.0, target_error=target_error)\n",
        "        elif name == 'One Cycle LR':\n",
        "            epochs = train_model(model, optimizer_class, lr_scheduler=lambda opt: optim.lr_scheduler.OneCycleLR(opt, max_lr=0.1, steps_per_epoch=len(dataloader), epochs=500), target_error=target_error)\n",
        "        else:\n",
        "            epochs = train_model(model, optimizer_class, target_error=target_error)\n",
        "        epochs_to_target.append(epochs)\n",
        "    avg_epochs = np.mean(epochs_to_target)\n",
        "    # Bootstrap confidence interval\n",
        "    bootstraps = [np.mean(resample(epochs_to_target)) for _ in range(1000)]\n",
        "    ci_lower, ci_upper = np.percentile(bootstraps, [2.5, 97.5])\n",
        "    results.append({'Strategy': name, 'Mean Epochs': avg_epochs, 'CI Lower': ci_lower, 'CI Upper': ci_upper})\n",
        "\n",
        "# Display results as a table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4Op62huPI9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}