{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJtY26XR2dQW4TR4yIaHYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/CS909/blob/master/autodiff_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building an Automatic Differentiation Library: Reverse-Mode (Backpropagation)**\n",
        "(Fayyaz Minhas)\n",
        "\n",
        "\n",
        "Automatic differentiation (**autodiff**) is a fundamental tool in modern machine learning and scientific computing.  \n",
        "\n",
        "Unlike **forward-mode autodiff**, which propagates derivatives forward using **dual numbers**, **reverse-mode autodiff** computes derivatives **backward** using the **chain rule**.  \n",
        "\n",
        "### **What You Will Learn**\n",
        "1. **How reverse-mode autodiff works**\n",
        "2. **Why reverse-mode is efficient for functions with many inputs**\n",
        "3. **How to implement reverse-mode autodiff from scratch in Python**\n",
        "4. **Computing gradients for**:\n",
        "\n",
        "$$\n",
        "e = (a + b)(b + 1)\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2KLgLxIh0D00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1Ô∏è‚É£ Understanding Reverse-Mode Autodiff**\n",
        "\n",
        "### **üîπ Why Use Reverse-Mode?**  \n",
        "Reverse-mode autodiff is useful for computing gradients **when a function has many inputs and one output**, such as **loss functions in deep learning**.  \n",
        "\n",
        "For a function:  \n",
        "$$\n",
        "e = f(a, b)\n",
        "$$\n",
        "Reverse-mode computes **all gradients** in a **single backward pass**, making it **much more efficient than forward-mode** for large-scale optimization problems.\n",
        "\n",
        "Historical note:\n",
        "\n",
        "While, Forward-mode differentiation has roots in dual numbers (1873) and was formalized in Wengert‚Äôs work (1964), Seppo Linnainmaa (1970) is credited with the first formal description of reverse-mode autodiff in his PhD dissertation, which introduced the idea of storing intermediate results and computing gradients backward.\n",
        "\n"
      ],
      "metadata": {
        "id": "jpee6nch0Liy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîπ How Reverse-Mode Works (Computation Graph)**\n",
        "\n",
        "Instead of using **dual numbers**, reverse-mode constructs a **computation graph** during the forward pass, then applies the **chain rule backward** to compute gradients efficiently.\n",
        "\n",
        "For the function:\n",
        "\n",
        "$$\n",
        "e = (a + b)(b + 1)\n",
        "$$\n",
        "\n",
        "At $ a = 2 $, $ b = 3 $, we compute:\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Forward Pass (Compute Function Value)**\n",
        "To break the function into intermediate steps:\n",
        "\n",
        "    a        b\n",
        "     \\      /  \n",
        "      \\    /\n",
        "        c     d\n",
        "         \\   /\n",
        "           e\n",
        "\n",
        "\n",
        "### **Step 1: Compute $ c $**\n",
        "$$\n",
        "c = a + b\n",
        "$$\n",
        "Substituting $ a = 2 $, $ b = 3 $:\n",
        "$$\n",
        "c = 2 + 3 = 5\n",
        "$$\n",
        "\n",
        "### **Step 2: Compute $ d $**\n",
        "$$\n",
        "d = b + 1\n",
        "$$\n",
        "Substituting $ b = 3 $:\n",
        "$$\n",
        "d = 3 + 1 = 4\n",
        "$$\n",
        "\n",
        "### **Step 3: Compute $ e $**\n",
        "$$\n",
        "e = c \\cdot d\n",
        "$$\n",
        "Substituting $ c = 5 $, $ d = 4 $:\n",
        "$$\n",
        "e = 5 \\times 4 = 20\n",
        "$$\n",
        "\n",
        "At this point, we have computed the final output value $ e = 20 $. Now, we compute the gradients by applying the **chain rule step by step**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Backward Pass (Applying Chain Rule)**\n",
        "Now, we compute the **gradients of $ e $ with respect to $ a $ and $ b $ using the chain rule**.\n",
        "\n",
        "We begin with derivative at the last step which is `e.grad`, i.e., derivative of the final value with respect to $e$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial e} = 1\n",
        "$$\n",
        "### **Step 1: Compute $ \\frac{\\partial e}{\\partial c} $ and $ \\frac{\\partial e}{\\partial d} $**\n",
        "Since:\n",
        "$$\n",
        "e = c \\cdot d\n",
        "$$\n",
        "Applying the derivative of a product gives us `c.grad`, i.e., derivative of the final value with respect to $c$ and `d.grad`, i.e., derivative of the final value with respect to $d$ as:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial c} = d, \\quad \\frac{\\partial e}{\\partial d} = c\n",
        "$$\n",
        "Substituting $ c = 5 $, $ d = 4 $:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial c} = 4, \\quad \\frac{\\partial e}{\\partial d} = 5\n",
        "$$\n",
        "\n",
        "### **Step 2: Compute $ \\frac{\\partial c}{\\partial a} $ and $ \\frac{\\partial c}{\\partial b} $**\n",
        "Since:\n",
        "$$\n",
        "c = a + b\n",
        "$$\n",
        "Differentiating:\n",
        "$$\n",
        "\\frac{\\partial c}{\\partial a} = 1, \\quad \\frac{\\partial c}{\\partial b} = 1\n",
        "$$\n",
        "\n",
        "### **Step 3: Compute $ \\frac{\\partial d}{\\partial b} $**\n",
        "Since:\n",
        "$$\n",
        "d = b + 1\n",
        "$$\n",
        "Differentiating:\n",
        "$$\n",
        "\\frac{\\partial d}{\\partial b} = 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Compute Gradients Using Chain Rule**\n",
        "Now, we apply the **chain rule** step by step.\n",
        "\n",
        "### **Computing $ \\frac{\\partial e}{\\partial a} $**\n",
        "\n",
        "Using the chain rule:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a}\n",
        "$$\n",
        "Substituting known values:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial a} = 4 \\times 1 = 4\n",
        "$$\n",
        "\n",
        "### **Computing $ \\frac{\\partial e}{\\partial b} $**\n",
        "\n",
        "Applying the chain rule for $ b $, since $ e $ depends on $ b $ through both $ c $ and $ d $:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial b} = \\frac{\\partial e}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} + \\frac{\\partial e}{\\partial d} \\cdot \\frac{\\partial d}{\\partial b}\n",
        "$$\n",
        "Substituting known values:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial b} = (4 \\times 1) + (5 \\times 1) = 4 + 5 = 9\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **Final Gradients**\n",
        "Thus, the computed gradients are:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial a} = 4, \\quad \\frac{\\partial e}{\\partial b} = 9\n",
        "$$\n",
        "\n",
        "‚úÖ **Reverse-mode autodiff efficiently computes all gradients in one backward pass!** üöÄ\n"
      ],
      "metadata": {
        "id": "IJPuFapS0NPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2Ô∏è‚É£ Implementing Reverse-Mode Autodiff in Python**\n",
        "We will now implement reverse-mode autodiff step by step.\n",
        "The `Tensor` class provides the foundation for reverse-mode automatic differentiation by **managing both values and their gradients** while dynamically building a computation graph. Instead of computing derivatives manually, this implementation **attaches a small function to each operation** that describes how its result depends on its inputs and how the gradients should propagate when differentiation is performed. Each operation, like addition or multiplication, registers **how it should compute its own derivative**, ensuring that when `.backward()` is called, the system can **traverse the computation graph in reverse**, applying the **chain rule** recursively. This allows complex functions to be differentiated automatically by **tracking dependencies** rather than symbolically solving for derivatives.\n",
        "\n",
        "You are welcome to try and add other operators to it.\n",
        "\n"
      ],
      "metadata": {
        "id": "xZ-sfWno0Yor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffScalar:\n",
        "    def __init__(self, value, requires_grad=False):\n",
        "        \"\"\"\n",
        "        Initialize a scalar value that supports automatic differentiation.\n",
        "\n",
        "        Arguments:\n",
        "        - value: The numerical value of the scalar.\n",
        "        - requires_grad: Whether this scalar requires gradient computation.\n",
        "\n",
        "        Attributes:\n",
        "        - grad: Stores the gradient of L (final output) wrt to this variable (set to 0 if differentiation is needed).\n",
        "        - _backward: A function that defines how gradients should be propagated.\n",
        "        - _prev: Tracks the dependent variables (parents in computation graph).\n",
        "        \"\"\"\n",
        "        self.value = value\n",
        "        self.requires_grad = requires_grad\n",
        "        self.grad = 0 if requires_grad else None  # Initialize gradient\n",
        "        self._backward = lambda: None  # Placeholder for gradient propagation\n",
        "        self._prev = set()  # Store references to parent nodes in computation graph\n",
        "\n",
        "    def __add__(self, other):\n",
        "        \"\"\"\n",
        "        Define addition operation while maintaining the computation graph.\n",
        "\n",
        "        If self and other are differentiable (requires_grad=True),\n",
        "        then their gradients must be tracked for backpropagation.\n",
        "        \"\"\"\n",
        "        other = other if isinstance(other, DiffScalar) else DiffScalar(other)  # Ensure other is a DiffScalar\n",
        "        out = DiffScalar(self.value + other.value, requires_grad=self.requires_grad or other.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            \"\"\"\n",
        "            Compute the gradient for self and other using the chain rule.\n",
        "\n",
        "            Since d/dx (x + y) = 1, we propagate out.grad directly to both.\n",
        "\n",
        "            We use += because the variable might be involved in multiple operations,\n",
        "            so its gradient accumulates contributions from multiple sources.\n",
        "            \"\"\"\n",
        "            if self.requires_grad:\n",
        "                self.grad += out.grad  # ‚àÇL/‚àÇself += ‚àÇL/‚àÇout * ‚àÇout/‚àÇself (which is 1)\n",
        "            if other.requires_grad:\n",
        "                other.grad += out.grad  # ‚àÇL/‚àÇother += ‚àÇL/‚àÇout * ‚àÇout/‚àÇother (which is 1)\n",
        "\n",
        "        out._backward = _backward  # Store the backward function for this operation\n",
        "        out._prev = {self, other}  # Track dependencies\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        \"\"\"\n",
        "        Define multiplication operation while maintaining the computation graph.\n",
        "\n",
        "        If self and other are differentiable (requires_grad=True),\n",
        "        then their gradients must be tracked for backpropagation.\n",
        "        \"\"\"\n",
        "        other = other if isinstance(other, DiffScalar) else DiffScalar(other)  # Ensure other is a DiffScalar\n",
        "        out = DiffScalar(self.value * other.value, requires_grad=self.requires_grad or other.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            \"\"\"\n",
        "            Compute the gradient for self and other using the chain rule.\n",
        "\n",
        "            Since d/dx (x * y) = y and d/dy (x * y) = x, we propagate:\n",
        "                - other.value * out.grad to self.grad\n",
        "                - self.value * out.grad to other.grad\n",
        "\n",
        "            We use += because a variable might be used in multiple operations,\n",
        "            meaning its gradient needs to accumulate from multiple contributions.\n",
        "            \"\"\"\n",
        "            if self.requires_grad:\n",
        "                self.grad += other.value * out.grad  # ‚àÇL/‚àÇself += ‚àÇL/‚àÇout * ‚àÇout/‚àÇself (which is other.value)\n",
        "            if other.requires_grad:\n",
        "                other.grad += self.value * out.grad  # ‚àÇL/‚àÇother += ‚àÇL/‚àÇout * ‚àÇout/‚àÇother (which is self.value)\n",
        "\n",
        "        out._backward = _backward  # Store the backward function for this operation\n",
        "        out._prev = {self, other}  # Track dependencies\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Perform backpropagation to compute gradients of all dependent variables.\n",
        "        \"\"\"\n",
        "        # Reset all gradients in the computation graph\n",
        "        def reset_grads(node):\n",
        "            if node.grad is not None:\n",
        "                node.grad = 0\n",
        "            for parent in node._prev:\n",
        "                reset_grads(parent)\n",
        "\n",
        "        reset_grads(self)  # Clear previous gradients\n",
        "        self.grad = 1  # Seed gradient for the output (‚àÇL/‚àÇself = 1)\n",
        "\n",
        "        topo = []  # Store nodes in topological order\n",
        "        visited = set()\n",
        "\n",
        "        def build_topo(node):\n",
        "            \"\"\" Perform depth-first traversal to build a valid topological order \"\"\"\n",
        "            if node not in visited:\n",
        "                visited.add(node)\n",
        "                for parent in node._prev:\n",
        "                    build_topo(parent)\n",
        "                topo.append(node)\n",
        "\n",
        "        build_topo(self)  # Construct topological order\n",
        "\n",
        "        for node in reversed(topo):\n",
        "            node._backward()  # Apply stored _backward function to propagate gradients\n",
        "\n",
        "# Define a and b\n",
        "a = DiffScalar(2, requires_grad=True)\n",
        "b = DiffScalar(3, requires_grad=True)\n",
        "\n",
        "# Compute e = (a + b) * (b + 1)\n",
        "e = (a + b) * (b + 1)\n",
        "\n",
        "# Perform backward pass\n",
        "e.backward()\n",
        "\n",
        "# Print gradients\n",
        "print(a.grad)\n",
        "print(b.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5GVOyRF0aHN",
        "outputId": "c90e5714-9021-4851-e135-ca295a1d1dae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare to pyTorch using the same example:"
      ],
      "metadata": {
        "id": "id64bn3iakfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define scalars with requires_grad=True to track gradients\n",
        "a = torch.tensor(2.0, requires_grad=True)  # Equivalent to DiffScalar(2, requires_grad=True)\n",
        "b = torch.tensor(3.0, requires_grad=True)  # Equivalent to DiffScalar(1, requires_grad=True)\n",
        "\n",
        "# Compute e = (a + b) * (b + 1)\n",
        "e = (a + b) * (b + 1)  # PyTorch automatically tracks computation\n",
        "\n",
        "# Perform backward pass\n",
        "e.backward()  # Computes gradients\n",
        "\n",
        "# Print gradients\n",
        "print(a.grad)\n",
        "print(b.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei98wRQ91KlD",
        "outputId": "cabf9a95-dd7b-4e09-a80f-e6c24b901c7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n",
            "tensor(9.)\n"
          ]
        }
      ]
    }
  ]
}