{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG2xYKFb/jBO5P2LHSwd8B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/CS909/blob/master/nn_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Optimization and Convergence Issues in Neural Networks**\n",
        "(Fayyaz Minhas)\n",
        "\n",
        "## **1. Introduction to the Chain Rule**\n",
        "### **What is the Chain Rule?**\n",
        "The **chain rule** is a fundamental rule in calculus that allows us to differentiate composite functions. If a function $f$ depends on $u$, and $u$ depends on $x$, then we can express its output as:\n",
        "\n",
        "$$\n",
        "z = f(u), \\quad u = g(x)\n",
        "$$\n",
        "\n",
        "The chain rule states that:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x}\n",
        "$$\n",
        "\n",
        "This tells us that to find the derivative of $z$ with respect to $x$, we must **multiply** the derivative of $z$ with respect to $u$ by the derivative of $u$ with respect to $x$.\n",
        "\n",
        "### **Why is the Chain Rule Important?**\n",
        "- It allows us to **compute derivatives of nested functions**.\n",
        "- It is **fundamental in backpropagation**, as each layer in a neural network depends on the previous one.\n",
        "\n",
        "### **Example: Applying the Chain Rule**\n",
        "Let's say:\n",
        "\n",
        "$$\n",
        "z = (3x + 1)^2\n",
        "$$\n",
        "\n",
        "We define:\n",
        "\n",
        "$$\n",
        "z = 3x + 1, \\quad z = u^2\n",
        "$$\n",
        "\n",
        "Now, applying the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x}\n",
        "$$\n",
        "\n",
        "Computing the derivatives:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial u} = 2u, \\quad \\frac{\\partial u}{\\partial x} = 3\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = 2(3x + 1) \\cdot 3 = 6(3x + 1)\n",
        "$$\n",
        "\n",
        "This same principle **applies to neural networks**, where each layer is dependent on the previous one, and **backpropagation uses the chain rule** to compute gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Neural Network Representation and Evaluation**\n",
        "We define a deep neural network with $L$ layers, where each layer applies its own activation function $a_k(.)$. The network output is:\n",
        "\n",
        "$$\n",
        "f(x; W) = a_L(W_L (...(a_2(W_2 (a_1(W_1 x)))))).\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $W_k$ is the weight matrix at layer $k$.\n",
        "- $a_k(.)$ is the activation function applied at layer $k$.\n",
        "\n",
        "Each layer transforms the input recursively:\n",
        "- The **pre-activation** at layer $k$ is:\n",
        "  \n",
        "  $$\n",
        "  z_k = W_k a_{k-1}\n",
        "  $$\n",
        "\n",
        "- The **post-activation** (output of the activation function) at layer $k$ is:\n",
        "\n",
        "  $$\n",
        "  a_k = a_k(z_k)\n",
        "  $$\n",
        "\n",
        "- The final output of the network is:\n",
        "\n",
        "  $$\n",
        "  f(x; W) = a_L(W_L a_{L-1}).\n",
        "  $$\n",
        "\n",
        "For **Evaluation**, we can use the squared error loss function:\n",
        "\n",
        "$$\n",
        "e = \\frac{1}{2} (f - y)^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Optimization: Compute the Gradient of the Error Using the Chain Rule**\n",
        "\n",
        "We aim to compute the gradient of the error function with respect to each layer's weight matrix $W_k$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial W_k}\n",
        "$$\n",
        "\n",
        "### **Step 1: Compute the Derivative of the Error Function**\n",
        "\n",
        "Applying the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial W_L} = \\frac{\\partial e}{\\partial f} \\cdot \\frac{\\partial f}{\\partial W_L}.\n",
        "$$\n",
        "\n",
        "Differentiating:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial f} = (f - y)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Since the final output is:\n",
        "\n",
        "$$\n",
        "f = a_L(W_L a_{L-1})\n",
        "$$\n",
        "\n",
        "we differentiate with respect to $W_L$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial W_L} = a_L'(z_L) \\cdot a_{L-1}.\n",
        "$$\n",
        "\n",
        "Thus, substituting in the chain rule, we get:\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial W_L} = (f - y) \\odot a_L'(z_L) a_{L-1}^T.\n",
        "$$\n",
        "\n",
        "\n",
        "We define the **error signal** at the output layer:\n",
        "\n",
        "$$\n",
        "\\delta_L = (f - y) \\odot a_L'(z_L).\n",
        "$$\n",
        "\n",
        "Substituting:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial W_L} = \\delta_L a_{L-1}^T.\n",
        "$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Compute the Gradient for Hidden Layers ($W_k$, for $k = L-1, ..., 1$)**\n",
        "\n",
        "Using the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial W_k} = \\frac{\\partial e}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial W_k}.\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial e}{\\partial a_k} = W_{k+1}^T \\delta_{k+1}.\n",
        "   $$\n",
        "\n",
        "   We define:\n",
        "\n",
        "   $$\n",
        "   \\delta_k = (W_{k+1}^T \\delta_{k+1}) \\odot a_k'(z_k).\n",
        "   $$\n",
        "\n",
        "Leading to the **Final Gradient Expression**:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial e}{\\partial W_k} = \\delta_k a_{k-1}^T.\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Compute the Full Expression for $W_1$**\n",
        "\n",
        "Expanding recursively:\n",
        "\n",
        "$$\n",
        "\\delta_1 = (W_2^T W_3^T \\dots W_L^T (f - y)) \\odot a_1'(z_1) \\odot a_2'(z_2) \\dots \\odot a_L'(z_L).\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial W_1} = \\left(W_2^T W_3^T \\dots W_L^T (f - y) \\right) \\odot a_1'(z_1) \\odot a_2'(z_2) \\dots \\odot a_L'(z_L) \\cdot x^T.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Simplified Example for $L = 3$ with Single-Dimensional $x$ and $y$**\n",
        "\n",
        "Let:\n",
        "- $x$ and $y$ be scalars.\n",
        "- Each layer has **one weight**: $w_1, w_2, w_3$.\n",
        "\n",
        "### **Forward Pass:**\n",
        "$$\n",
        "z_1 = w_1 x, \\quad a_1 = a_1(z_1).\n",
        "$$\n",
        "\n",
        "$$\n",
        "z_2 = w_2 a_1, \\quad a_2 = a_2(z_2).\n",
        "$$\n",
        "\n",
        "$$\n",
        "z_3 = w_3 a_2, \\quad f = a_3(z_3).\n",
        "$$\n",
        "\n",
        "### **Backpropagation:**\n",
        "$$\n",
        "\\delta_3 = (f - y) a_3'(z_3).\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\delta_2 = w_3 \\delta_3 a_2'(z_2).\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\delta_1 = w_2 \\delta_2 a_1'(z_1).\n",
        "$$\n",
        "\n",
        "### **Full Expression for $ \\frac{\\partial e}{\\partial w_1} $:**\n",
        "Expanding $ \\delta_1 $:\n",
        "\n",
        "$$\n",
        "\\delta_1 = w_2 w_3 (f - y) a_3'(z_3) a_2'(z_2) a_1'(z_1).\n",
        "$$\n",
        "\n",
        "Thus, the **full gradient for $ w_1 $** is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial w_1} = w_2 w_3 (f - y) a_3'(z_3) a_2'(z_2) a_1'(z_1) x.\n",
        "$$\n",
        "\n",
        "This confirms:\n",
        "- **Gradient includes all weight matrices** up to the last layer.\n",
        "- **Activation function derivatives** appear at every layer.\n",
        "- **Input \\( x \\) and output error \\( f - y \\) appear**.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## **Understanding Neural Network Convergence Issues**\n",
        "\n",
        "Neural networks are trained using **gradient-based optimization**, such as **gradient descent**, which updates the weights using:\n",
        "\n",
        "$$\n",
        "W_k^{(t+1)} = W_k^{(t)} - \\eta \\frac{\\partial e}{\\partial W_k}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $t$ is the iteration number.\n",
        "- $\\eta$ is the learning rate.\n",
        "- $\\frac{\\partial e}{\\partial W_k}$ is the gradient.\n",
        "\n",
        "For proper convergence, we want **the gradient to be zero only when the error is zero**, i.e.,:\n",
        "\n",
        "$$\n",
        "f - y = 0 \\quad \\Rightarrow \\quad f = y\n",
        "$$\n",
        "\n",
        "However, the **gradient can also be zero** under the following problematic conditions:\n",
        "\n",
        "### **1. The Input $x$ is Zero**\n",
        "👉 **Solution**: Ensure good input representation using **feature scaling and normalization** (e.g., **Batch Normalization**).\n",
        "\n",
        "### **2. Weight Values Being Close to Zero**\n",
        "If the weight values are too small, then:\n",
        "\n",
        "$$\n",
        "z_k = W_k a_{k-1}\n",
        "$$\n",
        "\n",
        "will be close to zero, leading to **low activations and weak gradient updates**.  \n",
        "👉 **Solution**: Proper weight initialization (e.g., **Xavier or He initialization**).\n",
        "\n",
        "### **3. Activation Function Derivatives Being Zero**\n",
        "If any activation function has a **zero derivative**, the gradient flow **stops**.  \n",
        "For example, **ReLU** has:\n",
        "\n",
        "$$\n",
        "a'(z) =\n",
        "\\begin{cases}\n",
        "  1, & z > 0 \\\\\n",
        "  0, & z \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "If $z \\leq 0$, gradients vanish.  \n",
        "👉 **Solution**: Use **Leaky ReLU** or **ELU**.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Vanishing and Exploding Gradients**\n",
        "Since backpropagation involves **multiplying gradients through layers**, two extreme cases can occur:\n",
        "\n",
        "### **Vanishing Gradient Problem**\n",
        "If:\n",
        "\n",
        "$$\n",
        "\\delta_1 = (W_2^T W_3^T \\dots W_L^T (f - y)) \\odot a_1'(z_1) \\odot a_2'(z_2) \\dots \\odot a_L'(z_L)\n",
        "$$\n",
        "\n",
        "and if **$a_k'(z_k)$ is small**, the product of many small terms causes **gradients to vanish**.  \n",
        "👉 **Solution**: Use **ReLU-based activations**, **Batch Normalization** and/or **Layer Normalization**.\n",
        "\n",
        "### **Exploding Gradient Problem**\n",
        "If gradient updates become large due to any reason, then this leads to **unstable updates**.  \n",
        "👉 **Solution**: Use **gradient clipping** and proper **weight initialization**.\n",
        "\n",
        "\n",
        "These problems can also be mitigated by the use of **skip connections** (aka residual connections) which break-up the multiplication and provide a path for gradient flow thus improving optimization.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cr6zuMUYUwzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DdEmDk05mJrR"
      }
    }
  ]
}