{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaysDs6+GIAS8WgF6Dzk76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/CS909/blob/master/brevitas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dxLY6-VkBk53",
        "outputId": "51c943eb-ae58-4f7a-aea6-9df22670a711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting brevitas\n",
            "  Downloading brevitas-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting dependencies==2.0.1 (from brevitas)\n",
            "  Downloading dependencies-2.0.1-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.11/dist-packages (from brevitas) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from brevitas) (24.2)\n",
            "Collecting setuptools<70.0 (from brevitas)\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from brevitas) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from brevitas) (2.5.1+cu124)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from brevitas) (4.12.2)\n",
            "Collecting unfoldNd (from brevitas)\n",
            "  Downloading unfoldNd-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9.1->brevitas)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->brevitas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->brevitas) (3.0.2)\n",
            "Downloading brevitas-0.11.0-py3-none-any.whl (706 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.6/706.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dependencies-2.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Downloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unfoldNd-0.2.3-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: setuptools, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dependencies, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, unfoldNd, brevitas\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed brevitas-0.11.0 dependencies-2.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 setuptools-69.5.1 unfoldNd-0.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "5c93bc80a1b64dd49d1c659f804eb129"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install brevitas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import brevitas.nn as qnn\n",
        "from brevitas.quant import Int32Bias\n",
        "\n",
        "# XOR dataset\n",
        "X = torch.tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]], dtype=torch.float32)\n",
        "Y = torch.tensor([0, 1, 1, 0], dtype=torch.long)  # Classification labels\n",
        "\n",
        "# Create a DataLoader for the XOR dataset\n",
        "dataset = TensorDataset(X, Y)\n",
        "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Standard MLP Model (32-bit precision)\n",
        "class StandardXORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StandardXORNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 10, bias=True)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, 10, bias=True)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(10, 2, bias=True)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Quantized MLP Model using Brevitas (2-bit precision)\n",
        "class QuantizedXORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuantizedXORNet, self).__init__()\n",
        "\n",
        "        # Input quantization\n",
        "        self.quant_inp = qnn.QuantIdentity(bit_width=2, return_quant_tensor=True)\n",
        "\n",
        "        # Fully connected layers with quantization\n",
        "        self.fc1 = qnn.QuantLinear(2, 10, bias=True, weight_bit_width=2, bias_quant=Int32Bias)\n",
        "        self.relu1 = qnn.QuantReLU(bit_width=2, return_quant_tensor=True)\n",
        "\n",
        "        self.fc2 = qnn.QuantLinear(10, 10, bias=True, weight_bit_width=2, bias_quant=Int32Bias)\n",
        "        self.relu2 = qnn.QuantReLU(bit_width=2, return_quant_tensor=True)\n",
        "\n",
        "        self.fc3 = qnn.QuantLinear(10, 2, bias=True, weight_bit_width=2, bias_quant=Int32Bias)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant_inp(x)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate memory usage\n",
        "def calculate_model_memory_usage(model: nn.Module, input_size=(1, 2), bit_width=32, quantized=False):\n",
        "    memory_summary = {'parameters': 0, 'activations': 0, 'optimizer_states': 0}\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param_memory = param.numel() * bit_width / 8\n",
        "        memory_summary['parameters'] += param_memory\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        dummy_input = torch.randn(input_size)\n",
        "        hooks = []\n",
        "\n",
        "        def activation_hook(module, input, output):\n",
        "            if isinstance(output, torch.Tensor):\n",
        "                memory_summary['activations'] += output.numel() * bit_width / 8\n",
        "\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, (nn.ReLU, nn.Linear, qnn.QuantReLU, qnn.QuantLinear)):\n",
        "                hooks.append(module.register_forward_hook(activation_hook))\n",
        "\n",
        "        model(dummy_input)\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            state_memory = param.numel() * 2 * 32 / 8\n",
        "            memory_summary['optimizer_states'] += state_memory\n",
        "\n",
        "    total_memory = sum(memory_summary.values())\n",
        "    memory_summary['total_memory_bytes'] = total_memory\n",
        "    memory_summary['total_memory_kb'] = total_memory / 1024\n",
        "    memory_summary['total_memory_mb'] = total_memory / (1024 ** 2)\n",
        "\n",
        "    return memory_summary\n",
        "\n",
        "# Initialize both models\n",
        "standard_model = StandardXORNet()\n",
        "quantized_model = QuantizedXORNet()\n",
        "\n",
        "# Optimizers and Loss Function\n",
        "criterion = nn.NLLLoss()\n",
        "standard_optimizer = optim.Adam(standard_model.parameters(), lr=0.01)\n",
        "quantized_optimizer = optim.Adam(quantized_model.parameters(), lr=0.01)\n",
        "\n",
        "# Predictions before training\n",
        "def show_predictions(model, name, inputs, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        print(f\"\\n{name} Predictions:\")\n",
        "        print(\"Input:\\n\", inputs.numpy())\n",
        "        print(\"Predicted:\\n\", predicted.numpy())\n",
        "        print(\"Actual:\\n\", labels.numpy())\n",
        "        accuracy = (predicted == labels).float().mean().item() * 100\n",
        "        print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "show_predictions(standard_model, \"Standard Model (Before Training)\", X, Y)\n",
        "show_predictions(quantized_model, \"Quantized Model (Before Training)\", X, Y)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    for model, optimizer, loader in [(standard_model, standard_optimizer, train_loader),\n",
        "                                     (quantized_model, quantized_optimizer, train_loader)]:\n",
        "        model.train()\n",
        "        for data, target in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Predictions after training\n",
        "show_predictions(standard_model, \"Standard Model (After Training)\", X, Y)\n",
        "show_predictions(quantized_model, \"Quantized Model (After Training)\", X, Y)\n",
        "\n",
        "# Memory Usage Comparison\n",
        "standard_memory_usage = calculate_model_memory_usage(standard_model, bit_width=32, quantized=False)\n",
        "quantized_memory_usage = calculate_model_memory_usage(quantized_model, bit_width=2, quantized=True)\n",
        "\n",
        "print(\"\\nMemory Usage Comparison:\")\n",
        "print(\"\\nStandard Model (32-bit):\")\n",
        "for key, value in standard_memory_usage.items():\n",
        "    print(f\"{key}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nQuantized Model (2-bit, Brevitas):\")\n",
        "for key, value in quantized_memory_usage.items():\n",
        "    print(f\"{key}: {value:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtbxGpEnBsyd",
        "outputId": "631c62e0-825e-4bad-d588-67493127b80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Standard Model (Before Training) Predictions:\n",
            "Input:\n",
            " [[-1. -1.]\n",
            " [-1.  1.]\n",
            " [ 1. -1.]\n",
            " [ 1.  1.]]\n",
            "Predicted:\n",
            " [0 0 0 0]\n",
            "Actual:\n",
            " [0 1 1 0]\n",
            "Accuracy: 50.00%\n",
            "\n",
            "\n",
            "Quantized Model (Before Training) Predictions:\n",
            "Input:\n",
            " [[-1. -1.]\n",
            " [-1.  1.]\n",
            " [ 1. -1.]\n",
            " [ 1.  1.]]\n",
            "Predicted:\n",
            " [1 1 1 1]\n",
            "Actual:\n",
            " [0 1 1 0]\n",
            "Accuracy: 50.00%\n",
            "\n",
            "\n",
            "Standard Model (After Training) Predictions:\n",
            "Input:\n",
            " [[-1. -1.]\n",
            " [-1.  1.]\n",
            " [ 1. -1.]\n",
            " [ 1.  1.]]\n",
            "Predicted:\n",
            " [0 1 1 0]\n",
            "Actual:\n",
            " [0 1 1 0]\n",
            "Accuracy: 100.00%\n",
            "\n",
            "\n",
            "Quantized Model (After Training) Predictions:\n",
            "Input:\n",
            " [[-1. -1.]\n",
            " [-1.  1.]\n",
            " [ 1. -1.]\n",
            " [ 1.  1.]]\n",
            "Predicted:\n",
            " [0 1 1 0]\n",
            "Actual:\n",
            " [0 1 1 0]\n",
            "Accuracy: 100.00%\n",
            "\n",
            "\n",
            "Memory Usage Comparison:\n",
            "\n",
            "Standard Model (32-bit):\n",
            "parameters: 648.00\n",
            "activations: 168.00\n",
            "optimizer_states: 1296.00\n",
            "total_memory_bytes: 2112.00\n",
            "total_memory_kb: 2.06\n",
            "total_memory_mb: 0.00\n",
            "\n",
            "Quantized Model (2-bit, Brevitas):\n",
            "parameters: 41.25\n",
            "activations: 10.50\n",
            "optimizer_states: 1320.00\n",
            "total_memory_bytes: 1371.75\n",
            "total_memory_kb: 1.34\n",
            "total_memory_mb: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import brevitas.nn as qnn\n",
        "from brevitas.quant import Int32Bias\n",
        "\n",
        "# Standard MLP Model (32-bit precision)\n",
        "class StandardXORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StandardXORNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 10, bias=True)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, 10, bias=True)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(10, 2, bias=True)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Quantized MLP Model using Brevitas (2-bit precision)\n",
        "class QuantizedXORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuantizedXORNet, self).__init__()\n",
        "\n",
        "        # Input quantization\n",
        "        self.quant_inp = qnn.QuantIdentity(bit_width=2, return_quant_tensor=True)\n",
        "\n",
        "        # Fully connected layers with quantization\n",
        "        self.fc1 = qnn.QuantLinear(2, 10, bias=True, weight_bit_width=2, bias_quant=Int32Bias)\n",
        "        self.relu1 = qnn.QuantReLU(bit_width=2, return_quant_tensor=True)\n",
        "\n",
        "        self.fc2 = qnn.QuantLinear(10, 10, bias=True, weight_bit_width=2, bias_quant=Int32Bias)\n",
        "        self.relu2 = qnn.QuantReLU(bit_width=2, return_quant_tensor=True)\n",
        "\n",
        "        self.fc3 = qnn.QuantLinear(10, 2, bias=True, weight_bit_width=2, bias_quant=Int32Bias)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant_inp(x)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Function to measure actual memory usage\n",
        "def measure_actual_memory_usage(model, model_name):\n",
        "    total_memory = 0\n",
        "    print(f\"\\nActual Memory Usage for {model_name}:\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        param_memory = param.numel() * param.element_size()\n",
        "        total_memory += param_memory\n",
        "        print(f\"  {name}: {param.numel()} elements * {param.element_size()} bytes = {param_memory:.2f} bytes\")\n",
        "\n",
        "    for name, buffer in model.named_buffers():\n",
        "        buffer_memory = buffer.numel() * buffer.element_size()\n",
        "        total_memory += buffer_memory\n",
        "        print(f\"  {name} (buffer): {buffer.numel()} elements * {buffer.element_size()} bytes = {buffer_memory:.2f} bytes\")\n",
        "\n",
        "    print(f\"Total Model Memory Usage: {total_memory / 1024:.2f} KB ({total_memory / (1024 ** 2):.4f} MB)\")\n",
        "    return total_memory\n",
        "\n",
        "# Initialize models\n",
        "standard_model = StandardXORNet()\n",
        "quantized_model = QuantizedXORNet()\n",
        "\n",
        "# Measure memory usage of the models\n",
        "standard_memory = measure_actual_memory_usage(standard_model, \"Standard Model (32-bit)\")\n",
        "quantized_memory = measure_actual_memory_usage(quantized_model, \"Quantized Model (2-bit)\")\n",
        "\n",
        "# Comparison of total memory usage\n",
        "print(\"\\nMemory Usage Comparison:\")\n",
        "print(f\"Standard Model: {standard_memory / 1024:.2f} KB\")\n",
        "print(f\"Quantized Model: {quantized_memory / 1024:.2f} KB\")\n",
        "print(f\"Memory Reduction: {(1 - quantized_memory / standard_memory) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKtRDZpeFwaK",
        "outputId": "4ea15a22-2099-42eb-c2c8-e76d1763c606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Actual Memory Usage for Standard Model (32-bit):\n",
            "  fc1.weight: 20 elements * 4 bytes = 80.00 bytes\n",
            "  fc1.bias: 10 elements * 4 bytes = 40.00 bytes\n",
            "  fc2.weight: 100 elements * 4 bytes = 400.00 bytes\n",
            "  fc2.bias: 10 elements * 4 bytes = 40.00 bytes\n",
            "  fc3.weight: 20 elements * 4 bytes = 80.00 bytes\n",
            "  fc3.bias: 2 elements * 4 bytes = 8.00 bytes\n",
            "Total Model Memory Usage: 0.63 KB (0.0006 MB)\n",
            "\n",
            "Actual Memory Usage for Quantized Model (2-bit):\n",
            "  quant_inp.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value: 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.weight: 20 elements * 4 bytes = 80.00 bytes\n",
            "  fc1.bias: 10 elements * 4 bytes = 40.00 bytes\n",
            "  relu1.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value: 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.weight: 100 elements * 4 bytes = 400.00 bytes\n",
            "  fc2.bias: 10 elements * 4 bytes = 40.00 bytes\n",
            "  relu2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value: 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.weight: 20 elements * 4 bytes = 80.00 bytes\n",
            "  fc3.bias: 2 elements * 4 bytes = 8.00 bytes\n",
            "  quant_inp.input_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  quant_inp.act_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  quant_inp.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.buffer (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  quant_inp.act_quant.fused_activation_quant_proxy.tensor_quant.zero_point_impl.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  quant_inp.act_quant.fused_activation_quant_proxy.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.input_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.output_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.weight_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.weight_quant.tensor_quant.zero_point_impl.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.weight_quant.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.bias_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.bias_quant.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc1.bias_quant.tensor_quant.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu1.input_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu1.act_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu1.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.buffer (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu1.act_quant.fused_activation_quant_proxy.tensor_quant.zero_point_impl.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu1.act_quant.fused_activation_quant_proxy.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.input_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.output_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.weight_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.weight_quant.tensor_quant.zero_point_impl.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.weight_quant.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.bias_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.bias_quant.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc2.bias_quant.tensor_quant.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu2.input_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu2.act_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.buffer (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu2.act_quant.fused_activation_quant_proxy.tensor_quant.zero_point_impl.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  relu2.act_quant.fused_activation_quant_proxy.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.input_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.output_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.weight_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.weight_quant.tensor_quant.zero_point_impl.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.weight_quant.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.bias_quant._zero_hw_sentinel.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.bias_quant.tensor_quant.msb_clamp_bit_width_impl.bit_width.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "  fc3.bias_quant.tensor_quant.zero_point.value (buffer): 1 elements * 4 bytes = 4.00 bytes\n",
            "Total Model Memory Usage: 0.80 KB (0.0008 MB)\n",
            "\n",
            "Memory Usage Comparison:\n",
            "Standard Model: 0.63 KB\n",
            "Quantized Model: 0.80 KB\n",
            "Memory Reduction: -25.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.quantization import (\n",
        "    QuantStub, DeQuantStub, prepare_qat, convert, get_default_qat_qconfig\n",
        ")\n",
        "\n",
        "# XOR dataset\n",
        "X = torch.tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]], dtype=torch.float32)\n",
        "Y = torch.tensor([0, 1, 1, 0], dtype=torch.long)  # Classification labels\n",
        "\n",
        "# Create a DataLoader for the XOR dataset\n",
        "dataset = TensorDataset(X, Y)\n",
        "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Simple MLP Model with Quantization Support\n",
        "class QuantizedXORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuantizedXORNet, self).__init__()\n",
        "\n",
        "        # Define a simple fully connected network\n",
        "        self.fc1 = nn.Linear(2, 10)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(10, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        # Quantization Stubs\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Quantize input\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        # Dequantize output\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = QuantizedXORNet()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Prepare the model for Quantization-Aware Training (QAT)\n",
        "model.qconfig = get_default_qat_qconfig('fbgemm')  # 'fbgemm' is recommended for x86 CPUs\n",
        "prepare_qat(model, inplace=True)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Convert the model to a quantized version\n",
        "model.eval()\n",
        "model_int8 = convert(model.eval(), inplace=False)\n",
        "\n",
        "# Function to show predictions\n",
        "def show_predictions(model, model_name, inputs, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        accuracy = (predicted == labels).float().mean().item() * 100\n",
        "\n",
        "        print(f\"\\n{model_name} Predictions:\")\n",
        "        print(\"Input:\\n\", inputs.numpy())\n",
        "        print(\"Predicted:\\n\", predicted.numpy())\n",
        "        print(\"Actual:\\n\", labels.numpy())\n",
        "        print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "# Show predictions before and after quantization\n",
        "show_predictions(model, \"Model Before Quantization\", X, Y)\n",
        "show_predictions(model_int8, \"Model After Quantization\", X, Y)\n",
        "\n",
        "# Measure memory usage of the quantized model\n",
        "def measure_model_memory(model, model_name):\n",
        "    total_memory = 0\n",
        "    print(f\"\\nMemory Usage for {model_name}:\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        param_memory = param.numel() * param.element_size()\n",
        "        total_memory += param_memory\n",
        "        print(f\"  {name}: {param.numel()} elements * {param.element_size()} bytes = {param_memory:.2f} bytes\")\n",
        "\n",
        "    for name, buffer in model.named_buffers():\n",
        "        buffer_memory = buffer.numel() * buffer.element_size()\n",
        "        total_memory += buffer_memory\n",
        "        print(f\"  {name} (buffer): {buffer.numel()} elements * {buffer.element_size()} bytes = {buffer_memory:.2f} bytes\")\n",
        "\n",
        "    print(f\"Total Model Memory Usage: {total_memory / 1024:.2f} KB ({total_memory / (1024 ** 2):.4f} MB)\")\n",
        "\n",
        "# Measure memory usage before and after quantization\n",
        "measure_model_memory(model, \"Model Before Quantization\")\n",
        "measure_model_memory(model_int8, \"Model After Quantization\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0yZuVPdZJDWF",
        "outputId": "ac64cd5d-502c-4140-e90f-0d77c22a9ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Before Quantization Predictions:\n",
            "Input:\n",
            " [[-1. -1.]\n",
            " [-1.  1.]\n",
            " [ 1. -1.]\n",
            " [ 1.  1.]]\n",
            "Predicted:\n",
            " [0 1 1 0]\n",
            "Actual:\n",
            " [0 1 1 0]\n",
            "Accuracy: 100.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Could not run 'aten::_log_softmax.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_log_softmax.out' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44679 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_3.cpp:26243 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5390 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-af71c2e86155>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Show predictions before and after quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mshow_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model Before Quantization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mshow_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_int8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model After Quantization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Measure memory usage of the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-af71c2e86155>\u001b[0m in \u001b[0;36mshow_predictions\u001b[0;34m(model, model_name, inputs, labels)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-af71c2e86155>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Dequantize output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2246\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2248\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2249\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_log_softmax.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_log_softmax.out' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44679 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_3.cpp:26243 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5390 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNqZ40w1KCRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}